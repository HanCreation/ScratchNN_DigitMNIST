{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit MNIST Classifier\n",
    "\n",
    "Torch version.\n",
    "\n",
    "Found out that torch and numpy have similar syntax, so I try to create another version with torch and with some support to GPU compute (Kinda overkill lol)\n",
    "\n",
    "Coded and Created by Han Summer 2024\n",
    "\n",
    "Part of The 20th Summer Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  pixel10  pixel11  pixel12  pixel13  pixel14  pixel15  pixel16  pixel17  pixel18  pixel19  pixel20  pixel21  pixel22  pixel23  pixel24  pixel25  pixel26  pixel27  pixel28  pixel29  pixel30  pixel31  pixel32  pixel33  pixel34  pixel35  pixel36  pixel37  pixel38  pixel39  pixel40  pixel41  pixel42  pixel43  pixel44  pixel45  pixel46  pixel47  pixel48  pixel49  pixel50  pixel51  pixel52  pixel53  pixel54  pixel55  pixel56  pixel57  pixel58  pixel59  pixel60  pixel61  pixel62  pixel63  pixel64  pixel65  pixel66  pixel67  pixel68  pixel69  pixel70  pixel71  pixel72  pixel73  pixel74  pixel75  pixel76  pixel77  pixel78  pixel79  pixel80  pixel81  pixel82  pixel83  pixel84  pixel85  pixel86  pixel87  pixel88  pixel89  pixel90  pixel91  pixel92  pixel93  pixel94  pixel95  pixel96  pixel97  pixel98  pixel99  pixel100  pixel101  pixel102  pixel103  pixel104  pixel105  pixel106  pixel107  pixel108  pixel109  pixel110  pixel111  pixel112  pixel113  pixel114  pixel115  pixel116  pixel117  pixel118  pixel119  pixel120  pixel121  pixel122  pixel123  pixel124  pixel125  pixel126  pixel127  pixel128  pixel129  pixel130  pixel131  pixel132  pixel133  pixel134  pixel135  pixel136  pixel137  pixel138  pixel139  pixel140  pixel141  pixel142  pixel143  pixel144  pixel145  pixel146  pixel147  pixel148  pixel149  pixel150  pixel151  pixel152  pixel153  pixel154  pixel155  pixel156  pixel157  pixel158  pixel159  pixel160  pixel161  pixel162  pixel163  pixel164  pixel165  pixel166  pixel167  pixel168  pixel169  pixel170  pixel171  pixel172  pixel173  pixel174  pixel175  pixel176  pixel177  pixel178  pixel179  pixel180  pixel181  pixel182  pixel183  pixel184  pixel185  pixel186  pixel187  pixel188  pixel189  pixel190  pixel191  pixel192  pixel193  pixel194  pixel195  pixel196  pixel197  pixel198  pixel199  pixel200  pixel201  pixel202  pixel203  pixel204  pixel205  pixel206  pixel207  pixel208  pixel209  pixel210  pixel211  pixel212  pixel213  pixel214  pixel215  pixel216  pixel217  pixel218  pixel219  pixel220  pixel221  pixel222  pixel223  pixel224  pixel225  pixel226  pixel227  pixel228  pixel229  pixel230  pixel231  pixel232  pixel233  pixel234  pixel235  pixel236  pixel237  pixel238  pixel239  pixel240  pixel241  pixel242  pixel243  pixel244  pixel245  pixel246  pixel247  pixel248  pixel249  pixel250  pixel251  pixel252  pixel253  pixel254  pixel255  pixel256  pixel257  pixel258  pixel259  pixel260  pixel261  pixel262  pixel263  pixel264  pixel265  pixel266  pixel267  pixel268  pixel269  pixel270  pixel271  pixel272  pixel273  pixel274  pixel275  pixel276  pixel277  pixel278  pixel279  pixel280  pixel281  pixel282  pixel283  pixel284  pixel285  pixel286  pixel287  pixel288  pixel289  pixel290  pixel291  pixel292  pixel293  pixel294  pixel295  pixel296  pixel297  pixel298  pixel299  pixel300  pixel301  pixel302  pixel303  pixel304  pixel305  pixel306  pixel307  pixel308  pixel309  pixel310  pixel311  pixel312  pixel313  pixel314  pixel315  pixel316  pixel317  pixel318  pixel319  pixel320  pixel321  pixel322  pixel323  pixel324  pixel325  pixel326  pixel327  pixel328  pixel329  pixel330  pixel331  pixel332  pixel333  pixel334  pixel335  pixel336  pixel337  pixel338  pixel339  pixel340  pixel341  pixel342  pixel343  pixel344  pixel345  pixel346  pixel347  pixel348  pixel349  pixel350  pixel351  pixel352  pixel353  pixel354  pixel355  pixel356  pixel357  pixel358  pixel359  pixel360  pixel361  pixel362  pixel363  pixel364  pixel365  pixel366  pixel367  pixel368  pixel369  pixel370  pixel371  pixel372  pixel373  pixel374  pixel375  pixel376  pixel377  pixel378  pixel379  pixel380  pixel381  pixel382  pixel383  pixel384  pixel385  pixel386  pixel387  pixel388  pixel389  pixel390  pixel391  pixel392  pixel393  pixel394  pixel395  pixel396  pixel397  pixel398  pixel399  pixel400  pixel401  pixel402  pixel403  pixel404  pixel405  pixel406  pixel407  pixel408  pixel409  pixel410  pixel411  pixel412  pixel413  pixel414  pixel415  pixel416  pixel417  pixel418  pixel419  pixel420  pixel421  pixel422  pixel423  pixel424  pixel425  pixel426  pixel427  pixel428  pixel429  pixel430  pixel431  pixel432  pixel433  pixel434  pixel435  pixel436  pixel437  pixel438  pixel439  pixel440  pixel441  pixel442  pixel443  pixel444  pixel445  pixel446  pixel447  pixel448  pixel449  pixel450  pixel451  pixel452  pixel453  pixel454  pixel455  pixel456  pixel457  pixel458  pixel459  pixel460  pixel461  pixel462  pixel463  pixel464  pixel465  pixel466  pixel467  pixel468  pixel469  pixel470  pixel471  pixel472  pixel473  pixel474  pixel475  pixel476  pixel477  pixel478  pixel479  pixel480  pixel481  pixel482  pixel483  pixel484  pixel485  pixel486  pixel487  pixel488  pixel489  pixel490  pixel491  pixel492  pixel493  pixel494  pixel495  pixel496  pixel497  pixel498  pixel499  pixel500  pixel501  pixel502  pixel503  pixel504  pixel505  pixel506  pixel507  pixel508  pixel509  pixel510  pixel511  pixel512  pixel513  pixel514  pixel515  pixel516  pixel517  pixel518  pixel519  pixel520  pixel521  pixel522  pixel523  pixel524  pixel525  pixel526  pixel527  pixel528  pixel529  pixel530  pixel531  pixel532  pixel533  pixel534  pixel535  pixel536  pixel537  pixel538  pixel539  pixel540  pixel541  pixel542  pixel543  pixel544  pixel545  pixel546  pixel547  pixel548  pixel549  pixel550  pixel551  pixel552  pixel553  pixel554  pixel555  pixel556  pixel557  pixel558  pixel559  pixel560  pixel561  pixel562  pixel563  pixel564  pixel565  pixel566  pixel567  pixel568  pixel569  pixel570  pixel571  pixel572  pixel573  pixel574  pixel575  pixel576  pixel577  pixel578  pixel579  pixel580  pixel581  pixel582  pixel583  pixel584  pixel585  pixel586  pixel587  pixel588  pixel589  pixel590  pixel591  pixel592  pixel593  pixel594  pixel595  pixel596  pixel597  pixel598  pixel599  pixel600  pixel601  pixel602  pixel603  pixel604  pixel605  pixel606  pixel607  pixel608  pixel609  pixel610  pixel611  pixel612  pixel613  pixel614  pixel615  pixel616  pixel617  pixel618  pixel619  pixel620  pixel621  pixel622  pixel623  pixel624  pixel625  pixel626  pixel627  pixel628  pixel629  pixel630  pixel631  pixel632  pixel633  pixel634  pixel635  pixel636  pixel637  pixel638  pixel639  pixel640  pixel641  pixel642  pixel643  pixel644  pixel645  pixel646  pixel647  pixel648  pixel649  pixel650  pixel651  pixel652  pixel653  pixel654  pixel655  pixel656  pixel657  pixel658  pixel659  pixel660  pixel661  pixel662  pixel663  pixel664  pixel665  pixel666  pixel667  pixel668  pixel669  pixel670  pixel671  pixel672  pixel673  pixel674  pixel675  pixel676  pixel677  pixel678  pixel679  pixel680  pixel681  pixel682  pixel683  pixel684  pixel685  pixel686  pixel687  pixel688  pixel689  pixel690  pixel691  pixel692  pixel693  pixel694  pixel695  pixel696  pixel697  pixel698  pixel699  pixel700  pixel701  pixel702  pixel703  pixel704  pixel705  pixel706  pixel707  pixel708  pixel709  pixel710  pixel711  pixel712  pixel713  pixel714  pixel715  pixel716  pixel717  pixel718  pixel719  pixel720  pixel721  pixel722  pixel723  pixel724  pixel725  pixel726  pixel727  pixel728  pixel729  pixel730  pixel731  pixel732  pixel733  pixel734  pixel735  pixel736  pixel737  pixel738  pixel739  pixel740  pixel741  pixel742  pixel743  pixel744  pixel745  pixel746  pixel747  pixel748  pixel749  pixel750  pixel751  pixel752  pixel753  pixel754  pixel755  pixel756  pixel757  pixel758  pixel759  pixel760  pixel761  pixel762  pixel763  pixel764  pixel765  pixel766  pixel767  pixel768  pixel769  pixel770  pixel771  pixel772  pixel773  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  pixel782  pixel783\n",
       "1      0       0       0       0       0       0       0       0       0       0       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         188       255       94        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         191       250       253       93        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         123       248       253       167       10        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         80        247       253       208       13        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         29        207       253       235       77        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         54        209       253       253       88        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         93        254       253       238       170       17        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         23        210       254       253       159       0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         16        209       253       254       240       81        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         27        253       253       254       13        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         20        206       254       254       198       7         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         168       253       253       196       7         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         20        203       253       248       76        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         22        188       253       245       93        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         103       253       253       191       0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         89        240       253       195       25        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         15        220       253       253       80        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         94        253       253       253       94        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         89        251       253       250       131       0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         214       218       95        0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42k train set, with flattened 28x28 image => 784\n",
    "\n",
    "Label -> 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=torch.tensor(df.values)\n",
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.float()\n",
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42000, 785])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move data to GPU\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data=data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([42000, 785]), 42000, 785)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size() , data.size(0) , data.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7621, 35051, 19439,  ..., 38186, 38679, 20111])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(data.size(0)) #it creates a shuffled sequence of row indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle tensor\n",
    "data=data[torch.randperm(data.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([785, 16800])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev=data[:16800].T\n",
    "data_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
      "         12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,\n",
      "         24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,\n",
      "         36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,\n",
      "         48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,\n",
      "         60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,\n",
      "         72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,\n",
      "         84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,\n",
      "         96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105., 106., 107.,\n",
      "        108., 109., 110., 111., 112., 113., 114., 115., 116., 117., 118., 119.,\n",
      "        120., 121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
      "        132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,\n",
      "        144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154., 155.,\n",
      "        156., 157., 158., 159., 160., 161., 162., 163., 164., 165., 166., 167.,\n",
      "        168., 169., 170., 171., 172., 173., 174., 175., 176., 177., 178., 179.,\n",
      "        180., 181., 182., 183., 184., 185., 186., 187., 188., 189., 190., 191.,\n",
      "        192., 193., 194., 195., 196., 197., 198., 199., 200., 201., 202., 203.,\n",
      "        204., 205., 206., 207., 208., 209., 210., 211., 212., 213., 214., 215.,\n",
      "        216., 217., 218., 219., 220., 221., 222., 223., 224., 225., 226., 227.,\n",
      "        228., 229., 230., 231., 232., 233., 234., 235., 236., 237., 238., 239.,\n",
      "        240., 241., 242., 243., 244., 245., 246., 247., 248., 249., 250., 251.,\n",
      "        252., 253., 254., 255.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Find unique values\n",
    "unique_values = torch.unique(data)\n",
    "\n",
    "# Step 4: Print unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 16800])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev=data_dev[0]\n",
    "X_dev=data_dev[1:]\n",
    "X_dev=X_dev/255.0 #normalize the data\n",
    "X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275, 0.0314,\n",
      "        0.0353, 0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588, 0.0627, 0.0667,\n",
      "        0.0706, 0.0745, 0.0784, 0.0824, 0.0863, 0.0902, 0.0941, 0.0980, 0.1020,\n",
      "        0.1059, 0.1098, 0.1137, 0.1176, 0.1216, 0.1255, 0.1294, 0.1333, 0.1373,\n",
      "        0.1412, 0.1451, 0.1490, 0.1529, 0.1569, 0.1608, 0.1647, 0.1686, 0.1725,\n",
      "        0.1765, 0.1804, 0.1843, 0.1882, 0.1922, 0.1961, 0.2000, 0.2039, 0.2078,\n",
      "        0.2118, 0.2157, 0.2196, 0.2235, 0.2275, 0.2314, 0.2353, 0.2392, 0.2431,\n",
      "        0.2471, 0.2510, 0.2549, 0.2588, 0.2627, 0.2667, 0.2706, 0.2745, 0.2784,\n",
      "        0.2824, 0.2863, 0.2902, 0.2941, 0.2980, 0.3020, 0.3059, 0.3098, 0.3137,\n",
      "        0.3176, 0.3216, 0.3255, 0.3294, 0.3333, 0.3373, 0.3412, 0.3451, 0.3490,\n",
      "        0.3529, 0.3569, 0.3608, 0.3647, 0.3686, 0.3725, 0.3765, 0.3804, 0.3843,\n",
      "        0.3882, 0.3922, 0.3961, 0.4000, 0.4039, 0.4078, 0.4118, 0.4157, 0.4196,\n",
      "        0.4235, 0.4275, 0.4314, 0.4353, 0.4392, 0.4431, 0.4471, 0.4510, 0.4549,\n",
      "        0.4588, 0.4627, 0.4667, 0.4706, 0.4745, 0.4784, 0.4824, 0.4863, 0.4902,\n",
      "        0.4941, 0.4980, 0.5020, 0.5059, 0.5098, 0.5137, 0.5176, 0.5216, 0.5255,\n",
      "        0.5294, 0.5333, 0.5373, 0.5412, 0.5451, 0.5490, 0.5529, 0.5569, 0.5608,\n",
      "        0.5647, 0.5686, 0.5725, 0.5765, 0.5804, 0.5843, 0.5882, 0.5922, 0.5961,\n",
      "        0.6000, 0.6039, 0.6078, 0.6118, 0.6157, 0.6196, 0.6235, 0.6275, 0.6314,\n",
      "        0.6353, 0.6392, 0.6431, 0.6471, 0.6510, 0.6549, 0.6588, 0.6627, 0.6667,\n",
      "        0.6706, 0.6745, 0.6784, 0.6824, 0.6863, 0.6902, 0.6941, 0.6980, 0.7020,\n",
      "        0.7059, 0.7098, 0.7137, 0.7176, 0.7216, 0.7255, 0.7294, 0.7333, 0.7373,\n",
      "        0.7412, 0.7451, 0.7490, 0.7529, 0.7569, 0.7608, 0.7647, 0.7686, 0.7725,\n",
      "        0.7765, 0.7804, 0.7843, 0.7882, 0.7922, 0.7961, 0.8000, 0.8039, 0.8078,\n",
      "        0.8118, 0.8157, 0.8196, 0.8235, 0.8275, 0.8314, 0.8353, 0.8392, 0.8431,\n",
      "        0.8471, 0.8510, 0.8549, 0.8588, 0.8627, 0.8667, 0.8706, 0.8745, 0.8784,\n",
      "        0.8824, 0.8863, 0.8902, 0.8941, 0.8980, 0.9020, 0.9059, 0.9098, 0.9137,\n",
      "        0.9176, 0.9216, 0.9255, 0.9294, 0.9333, 0.9373, 0.9412, 0.9451, 0.9490,\n",
      "        0.9529, 0.9569, 0.9608, 0.9647, 0.9686, 0.9725, 0.9765, 0.9804, 0.9843,\n",
      "        0.9882, 0.9922, 0.9961, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Find unique values\n",
    "unique_values = torch.unique(X_dev)\n",
    "\n",
    "# Step 4: Print unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the number is normalized\n",
    "X_dev.min() , X_dev.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 25200])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=data[16800:].T\n",
    "Y_train=data_train[0]\n",
    "X_train=data_train[1:]\n",
    "X_train=X_train/255.0\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275, 0.0314,\n",
      "        0.0353, 0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588, 0.0627, 0.0667,\n",
      "        0.0706, 0.0745, 0.0784, 0.0824, 0.0863, 0.0902, 0.0941, 0.0980, 0.1020,\n",
      "        0.1059, 0.1098, 0.1137, 0.1176, 0.1216, 0.1255, 0.1294, 0.1333, 0.1373,\n",
      "        0.1412, 0.1451, 0.1490, 0.1529, 0.1569, 0.1608, 0.1647, 0.1686, 0.1725,\n",
      "        0.1765, 0.1804, 0.1843, 0.1882, 0.1922, 0.1961, 0.2000, 0.2039, 0.2078,\n",
      "        0.2118, 0.2157, 0.2196, 0.2235, 0.2275, 0.2314, 0.2353, 0.2392, 0.2431,\n",
      "        0.2471, 0.2510, 0.2549, 0.2588, 0.2627, 0.2667, 0.2706, 0.2745, 0.2784,\n",
      "        0.2824, 0.2863, 0.2902, 0.2941, 0.2980, 0.3020, 0.3059, 0.3098, 0.3137,\n",
      "        0.3176, 0.3216, 0.3255, 0.3294, 0.3333, 0.3373, 0.3412, 0.3451, 0.3490,\n",
      "        0.3529, 0.3569, 0.3608, 0.3647, 0.3686, 0.3725, 0.3765, 0.3804, 0.3843,\n",
      "        0.3882, 0.3922, 0.3961, 0.4000, 0.4039, 0.4078, 0.4118, 0.4157, 0.4196,\n",
      "        0.4235, 0.4275, 0.4314, 0.4353, 0.4392, 0.4431, 0.4471, 0.4510, 0.4549,\n",
      "        0.4588, 0.4627, 0.4667, 0.4706, 0.4745, 0.4784, 0.4824, 0.4863, 0.4902,\n",
      "        0.4941, 0.4980, 0.5020, 0.5059, 0.5098, 0.5137, 0.5176, 0.5216, 0.5255,\n",
      "        0.5294, 0.5333, 0.5373, 0.5412, 0.5451, 0.5490, 0.5529, 0.5569, 0.5608,\n",
      "        0.5647, 0.5686, 0.5725, 0.5765, 0.5804, 0.5843, 0.5882, 0.5922, 0.5961,\n",
      "        0.6000, 0.6039, 0.6078, 0.6118, 0.6157, 0.6196, 0.6235, 0.6275, 0.6314,\n",
      "        0.6353, 0.6392, 0.6431, 0.6471, 0.6510, 0.6549, 0.6588, 0.6627, 0.6667,\n",
      "        0.6706, 0.6745, 0.6784, 0.6824, 0.6863, 0.6902, 0.6941, 0.6980, 0.7020,\n",
      "        0.7059, 0.7098, 0.7137, 0.7176, 0.7216, 0.7255, 0.7294, 0.7333, 0.7373,\n",
      "        0.7412, 0.7451, 0.7490, 0.7529, 0.7569, 0.7608, 0.7647, 0.7686, 0.7725,\n",
      "        0.7765, 0.7804, 0.7843, 0.7882, 0.7922, 0.7961, 0.8000, 0.8039, 0.8078,\n",
      "        0.8118, 0.8157, 0.8196, 0.8235, 0.8275, 0.8314, 0.8353, 0.8392, 0.8431,\n",
      "        0.8471, 0.8510, 0.8549, 0.8588, 0.8627, 0.8667, 0.8706, 0.8745, 0.8784,\n",
      "        0.8824, 0.8863, 0.8902, 0.8941, 0.8980, 0.9020, 0.9059, 0.9098, 0.9137,\n",
      "        0.9176, 0.9216, 0.9255, 0.9294, 0.9333, 0.9373, 0.9412, 0.9451, 0.9490,\n",
      "        0.9529, 0.9569, 0.9608, 0.9647, 0.9686, 0.9725, 0.9765, 0.9804, 0.9843,\n",
      "        0.9882, 0.9922, 0.9961, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Find unique values\n",
    "unique_values = torch.unique(X_train)\n",
    "\n",
    "# Step 4: Print unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting Y_train to int64 since class is not a float\n",
    "# And F.one_hot only receive torch.int64 tensor\n",
    "Y_train=Y_train.long()\n",
    "Y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 25200]), torch.Size([25200]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init params for 10 neurons and output layers\n",
    "def initialize_parameters():\n",
    "    #First layer -> input is 784 and there are 10 neurons\n",
    "    W1= torch.rand((10,784)) -0.5\n",
    "    b1= torch.rand((10,1))-0.5\n",
    "    #Second layer -> input is 10 and output is 10 since there are 10 classes\n",
    "    W2= torch.rand((10,10))-0.5\n",
    "    b2= torch.rand((10,1)) -0.5\n",
    "    #Move params to GPU\n",
    "    W1=W1.to(device)\n",
    "    b1=b1.to(device)\n",
    "    W2=W2.to(device)\n",
    "    b2=b2.to(device)\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried the **torch.randn** function for init the params but when backward pass there is a problem which when summing the Z it would return NaN. \n",
    "\n",
    "AI told this is the problem caused by:\n",
    "- Overflow or underflow: If the values in your tensor are extremely large or small, summing them can lead to numerical instability.\n",
    "- Invalid operations: Operations like dividing by zero or taking the logarithm of a negative number can produce NaN values.\n",
    "- Propagation of NaN: If your tensor already contains NaN values, any operation involving those values will result in NaN.\n",
    "- Gradients exploding or vanishing: In the context of neural networks, this can happen during backpropagation.\n",
    "- Infinities in the Tensor: If your tensor contains inf or -inf values, the sum operation could also return NaN. You can check for infinities using Z1.isinf().any().\n",
    "\n",
    "After I checked that the NaN value or Inf are not present in the Tensor, \n",
    "\n",
    "and I found out that it is **Vanishing/Exploding Gradients** problem: this **randn** initialization can cause gradients to become very small (vanishing) or very large (exploding) as they propagate through the network.\n",
    "\n",
    "how to fix it? Using Xavier/Glorot Init or He init(Good for ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward pass\n",
    "def ReLU(z):\n",
    "    return torch.max(torch.tensor(0),z)\n",
    "def Softmax(z):\n",
    "    \n",
    "    count= z.exp()\n",
    "    # print(count.shape)\n",
    "    prob= count/count.sum(0, keepdim=True)\n",
    "    # print(count.sum(0, keepdim=True))\n",
    "    return prob\n",
    "\n",
    "def forward(X,W1,b1,W2,b2):\n",
    "    # print( X.shape,W1.shape, b1.shape,W2.shape,b2.shape)\n",
    "    Z1=W1@X+b1 #Formely W1.dot(X)+b1 in numpy\n",
    "    A1=ReLU(Z1)\n",
    "    Z2=W2@A1+b2 #Formely W2.dot(A1)+b2 in numpy\n",
    "    A2=Softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check softmax\n",
    "z=torch.tensor([1.0,2.0,3.0])\n",
    "Softmax(z).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward pass\n",
    "def dReLU(z):\n",
    "    # print((z>0).float())\n",
    "    return (z>0).float()\n",
    "# dReLU(torch.tensor([-1,0,1])) #Checking Relu\n",
    "\n",
    "def backward(Z1,A1,Z2,A2,W2,X,Y):\n",
    "    # print(Z1,A1,Z2,A2,W2,X,Y)\n",
    "    m=Y_train.size(0)\n",
    "    one_hot_Y= F.one_hot(Y_train.long(), num_classes=10).T #Transpose the matrix, because the matrix should be each column as a sample of data\n",
    "    #--------------------\n",
    "    dZ2= A2-one_hot_Y\n",
    "    # print(dZ2)\n",
    "    dW2= 1/m*dZ2@(A1.T)\n",
    "    # print(dW2)\n",
    "    db2= (1/m)*(dZ2.sum(1,keepdim=True)) #Dont forget the keep dim\n",
    "    # print(db2,dZ2.sum(1,keepdim=True))\n",
    "    # print(dZ2.shape, dZ2.sum(1,keepdim=True).shape)\n",
    "    # print(db2.shape)\n",
    "    #--------------------\n",
    "    dZ1= (W2.T)@dZ2*dReLU(Z1)\n",
    "    # print(dZ1)\n",
    "    dW1= 1/m*dZ1@(X.T)\n",
    "    # print(dW1)\n",
    "    db1= (1/m)*(dZ1.sum(1,keepdim=True)) #Dont forget the keep dim\n",
    "    # print(dW1.shape, db1.shape, dW2.shape, db2.shape)\n",
    "    # print(db1,dZ1.sum(1,keepdim=True) )\n",
    "    # print(dZ1.shape, dZ1.sum(1,keepdim=True).shape)\n",
    "    # print(db1.shape)\n",
    "    return dW1,db1,dW2,db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update (W1,b1,W2,b2,dW1,db1,dW2,db2,lr):\n",
    "    W1=W1-lr*dW1\n",
    "    # print(b1.shape, db1.shape)\n",
    "    b1=b1-lr*db1\n",
    "    W2=W2-lr*dW2\n",
    "    # print(b2.shape, db2.shape)\n",
    "    b2=b2-lr*db2\n",
    "    # print(W1.shape, b1.shape, W2.shape, b2.shape)\n",
    "    # print(dW1,dW2,db1,db2)\n",
    "    # print(W1,b1,W2,b2)\n",
    "    return W1,b1,W2,b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def predict (A2):\n",
    "    #Return the index of the maximum value in the array\n",
    "    return torch.argmax(A2,0)\n",
    "\n",
    "def accuracy(Y,Y_pred):\n",
    "    return (Y==Y_pred).sum()/Y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,alpha,epochs):\n",
    "    W1,b1,W2,b2=initialize_parameters()\n",
    "    for i in range(epochs):\n",
    "        Z1,A1,Z2,A2=forward(X,W1,b1,W2,b2)\n",
    "        dW1,db1,dW2,db2=backward(Z1,A1,Z2,A2,W2,X,Y)\n",
    "        W1,b1,W2,b2=update(W1,b1,W2,b2,dW1,db1,dW2,db2,alpha)\n",
    "        # print(Z1,A1,Z2,A2)\n",
    "        # print(W1,b1,W2,b2)\n",
    "        if i%10==0: #Print the loss every 10 epochs\n",
    "            print('Iteration/Epoch: ',i)\n",
    "            acctrain=accuracy(predict(A2),Y)\n",
    "            print('Accuracy: ',acctrain.item())\n",
    "            Z1,A1,Z2,A2=forward(X_dev,W1,b1,W2,b2)\n",
    "            # print('Acc valll----------------------')\n",
    "            accval=accuracy(predict(A2),Y_dev).item()\n",
    "            print('Validation Accuracy: ',accval)\n",
    "            # print(Z1,A1,Z2,A2)\n",
    "            # print(W1,b1,W2,b2)\n",
    "            # if (abs(acctrain-accval)>0.01):\n",
    "            #     break\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration/Epoch:  0\n",
      "Accuracy:  0.07305555045604706\n",
      "Validation Accuracy:  0.07880952209234238\n",
      "Iteration/Epoch:  10\n",
      "Accuracy:  0.15813490748405457\n",
      "Validation Accuracy:  0.17613095045089722\n",
      "Iteration/Epoch:  20\n",
      "Accuracy:  0.28353172540664673\n",
      "Validation Accuracy:  0.28559523820877075\n",
      "Iteration/Epoch:  30\n",
      "Accuracy:  0.357103168964386\n",
      "Validation Accuracy:  0.35035714507102966\n",
      "Iteration/Epoch:  40\n",
      "Accuracy:  0.42293649911880493\n",
      "Validation Accuracy:  0.4151785671710968\n",
      "Iteration/Epoch:  50\n",
      "Accuracy:  0.47305554151535034\n",
      "Validation Accuracy:  0.4645237922668457\n",
      "Iteration/Epoch:  60\n",
      "Accuracy:  0.5140476226806641\n",
      "Validation Accuracy:  0.510357141494751\n",
      "Iteration/Epoch:  70\n",
      "Accuracy:  0.5515078902244568\n",
      "Validation Accuracy:  0.5468452572822571\n",
      "Iteration/Epoch:  80\n",
      "Accuracy:  0.5897618532180786\n",
      "Validation Accuracy:  0.5835714340209961\n",
      "Iteration/Epoch:  90\n",
      "Accuracy:  0.6231745481491089\n",
      "Validation Accuracy:  0.6193452477455139\n",
      "Iteration/Epoch:  100\n",
      "Accuracy:  0.6522618532180786\n",
      "Validation Accuracy:  0.6460714340209961\n",
      "Iteration/Epoch:  110\n",
      "Accuracy:  0.6764682531356812\n",
      "Validation Accuracy:  0.6727976202964783\n",
      "Iteration/Epoch:  120\n",
      "Accuracy:  0.6990079283714294\n",
      "Validation Accuracy:  0.6931547522544861\n",
      "Iteration/Epoch:  130\n",
      "Accuracy:  0.7166666388511658\n",
      "Validation Accuracy:  0.7113690376281738\n",
      "Iteration/Epoch:  140\n",
      "Accuracy:  0.7321428060531616\n",
      "Validation Accuracy:  0.7260118722915649\n",
      "Iteration/Epoch:  150\n",
      "Accuracy:  0.7454761862754822\n",
      "Validation Accuracy:  0.7374404668807983\n",
      "Iteration/Epoch:  160\n",
      "Accuracy:  0.7561507821083069\n",
      "Validation Accuracy:  0.7473214268684387\n",
      "Iteration/Epoch:  170\n",
      "Accuracy:  0.7651190161705017\n",
      "Validation Accuracy:  0.7572618722915649\n",
      "Iteration/Epoch:  180\n",
      "Accuracy:  0.7730951905250549\n",
      "Validation Accuracy:  0.7655357122421265\n",
      "Iteration/Epoch:  190\n",
      "Accuracy:  0.7807936072349548\n",
      "Validation Accuracy:  0.7738690376281738\n",
      "Iteration/Epoch:  200\n",
      "Accuracy:  0.7860317230224609\n",
      "Validation Accuracy:  0.7807142734527588\n",
      "Iteration/Epoch:  210\n",
      "Accuracy:  0.792023777961731\n",
      "Validation Accuracy:  0.786726176738739\n",
      "Iteration/Epoch:  220\n",
      "Accuracy:  0.7965078949928284\n",
      "Validation Accuracy:  0.7919642925262451\n",
      "Iteration/Epoch:  230\n",
      "Accuracy:  0.8013094663619995\n",
      "Validation Accuracy:  0.7972023487091064\n",
      "Iteration/Epoch:  240\n",
      "Accuracy:  0.805555522441864\n",
      "Validation Accuracy:  0.8015475869178772\n",
      "Iteration/Epoch:  250\n",
      "Accuracy:  0.810436487197876\n",
      "Validation Accuracy:  0.8055952191352844\n",
      "Iteration/Epoch:  260\n",
      "Accuracy:  0.8142063021659851\n",
      "Validation Accuracy:  0.8095238208770752\n",
      "Iteration/Epoch:  270\n",
      "Accuracy:  0.8171428442001343\n",
      "Validation Accuracy:  0.8139285445213318\n",
      "Iteration/Epoch:  280\n",
      "Accuracy:  0.8207539319992065\n",
      "Validation Accuracy:  0.8179166316986084\n",
      "Iteration/Epoch:  290\n",
      "Accuracy:  0.8241666555404663\n",
      "Validation Accuracy:  0.8205357193946838\n",
      "Iteration/Epoch:  300\n",
      "Accuracy:  0.8264682292938232\n",
      "Validation Accuracy:  0.823452353477478\n",
      "Iteration/Epoch:  310\n",
      "Accuracy:  0.8296031355857849\n",
      "Validation Accuracy:  0.8255357146263123\n",
      "Iteration/Epoch:  320\n",
      "Accuracy:  0.8323808908462524\n",
      "Validation Accuracy:  0.8280357122421265\n",
      "Iteration/Epoch:  330\n",
      "Accuracy:  0.8344841003417969\n",
      "Validation Accuracy:  0.8302381038665771\n",
      "Iteration/Epoch:  340\n",
      "Accuracy:  0.8367856740951538\n",
      "Validation Accuracy:  0.8324404954910278\n",
      "Iteration/Epoch:  350\n",
      "Accuracy:  0.8390079140663147\n",
      "Validation Accuracy:  0.8341071605682373\n",
      "Iteration/Epoch:  360\n",
      "Accuracy:  0.8409126400947571\n",
      "Validation Accuracy:  0.8365476131439209\n",
      "Iteration/Epoch:  370\n",
      "Accuracy:  0.8428968191146851\n",
      "Validation Accuracy:  0.8383928537368774\n",
      "Iteration/Epoch:  380\n",
      "Accuracy:  0.8448412418365479\n",
      "Validation Accuracy:  0.8408928513526917\n",
      "Iteration/Epoch:  390\n",
      "Accuracy:  0.8461507558822632\n",
      "Validation Accuracy:  0.8425595164299011\n",
      "Iteration/Epoch:  400\n",
      "Accuracy:  0.8474205732345581\n",
      "Validation Accuracy:  0.8441071510314941\n",
      "Iteration/Epoch:  410\n",
      "Accuracy:  0.8490476012229919\n",
      "Validation Accuracy:  0.8458333015441895\n",
      "Iteration/Epoch:  420\n",
      "Accuracy:  0.8506348729133606\n",
      "Validation Accuracy:  0.8476190567016602\n",
      "Iteration/Epoch:  430\n",
      "Accuracy:  0.8518253564834595\n",
      "Validation Accuracy:  0.8492857217788696\n",
      "Iteration/Epoch:  440\n",
      "Accuracy:  0.8530555367469788\n",
      "Validation Accuracy:  0.8501785397529602\n",
      "Iteration/Epoch:  450\n",
      "Accuracy:  0.8548015356063843\n",
      "Validation Accuracy:  0.8514285683631897\n",
      "Iteration/Epoch:  460\n",
      "Accuracy:  0.8557539582252502\n",
      "Validation Accuracy:  0.852321445941925\n",
      "Iteration/Epoch:  470\n",
      "Accuracy:  0.8567460179328918\n",
      "Validation Accuracy:  0.8535118699073792\n",
      "Iteration/Epoch:  480\n",
      "Accuracy:  0.8576189875602722\n",
      "Validation Accuracy:  0.8545833230018616\n",
      "Iteration/Epoch:  490\n",
      "Accuracy:  0.8590078949928284\n",
      "Validation Accuracy:  0.8553571105003357\n",
      "Iteration/Epoch:  500\n",
      "Accuracy:  0.8598015308380127\n",
      "Validation Accuracy:  0.856249988079071\n",
      "Iteration/Epoch:  510\n",
      "Accuracy:  0.8606745600700378\n",
      "Validation Accuracy:  0.8574404716491699\n",
      "Iteration/Epoch:  520\n",
      "Accuracy:  0.8617460131645203\n",
      "Validation Accuracy:  0.8583928346633911\n",
      "Iteration/Epoch:  530\n",
      "Accuracy:  0.8624602556228638\n",
      "Validation Accuracy:  0.8593452572822571\n",
      "Iteration/Epoch:  540\n",
      "Accuracy:  0.8635714054107666\n",
      "Validation Accuracy:  0.8600000143051147\n",
      "Iteration/Epoch:  550\n",
      "Accuracy:  0.8640872836112976\n",
      "Validation Accuracy:  0.8610714077949524\n",
      "Iteration/Epoch:  560\n",
      "Accuracy:  0.864960253238678\n",
      "Validation Accuracy:  0.8617857098579407\n",
      "Iteration/Epoch:  570\n",
      "Accuracy:  0.8654364943504333\n",
      "Validation Accuracy:  0.862500011920929\n",
      "Iteration/Epoch:  580\n",
      "Accuracy:  0.8661904335021973\n",
      "Validation Accuracy:  0.8627380728721619\n",
      "Iteration/Epoch:  590\n",
      "Accuracy:  0.8667857050895691\n",
      "Validation Accuracy:  0.8637499809265137\n",
      "Iteration/Epoch:  600\n",
      "Accuracy:  0.867539644241333\n",
      "Validation Accuracy:  0.8643452525138855\n",
      "Iteration/Epoch:  610\n",
      "Accuracy:  0.8679761290550232\n",
      "Validation Accuracy:  0.8645238280296326\n",
      "Iteration/Epoch:  620\n",
      "Accuracy:  0.868571400642395\n",
      "Validation Accuracy:  0.8651785850524902\n",
      "Iteration/Epoch:  630\n",
      "Accuracy:  0.8689681887626648\n",
      "Validation Accuracy:  0.8659523725509644\n",
      "Iteration/Epoch:  640\n",
      "Accuracy:  0.8693253397941589\n",
      "Validation Accuracy:  0.8669047355651855\n",
      "Iteration/Epoch:  650\n",
      "Accuracy:  0.8701983690261841\n",
      "Validation Accuracy:  0.8675594925880432\n",
      "Iteration/Epoch:  660\n",
      "Accuracy:  0.8709523677825928\n",
      "Validation Accuracy:  0.8679166436195374\n",
      "Iteration/Epoch:  670\n",
      "Accuracy:  0.8716269731521606\n",
      "Validation Accuracy:  0.8682142496109009\n",
      "Iteration/Epoch:  680\n",
      "Accuracy:  0.872420608997345\n",
      "Validation Accuracy:  0.8688095211982727\n",
      "Iteration/Epoch:  690\n",
      "Accuracy:  0.8730555176734924\n",
      "Validation Accuracy:  0.8694047331809998\n",
      "Iteration/Epoch:  700\n",
      "Accuracy:  0.8736904263496399\n",
      "Validation Accuracy:  0.869940459728241\n",
      "Iteration/Epoch:  710\n",
      "Accuracy:  0.8742856979370117\n",
      "Validation Accuracy:  0.8707142472267151\n",
      "Iteration/Epoch:  720\n",
      "Accuracy:  0.8744840621948242\n",
      "Validation Accuracy:  0.8707142472267151\n",
      "Iteration/Epoch:  730\n",
      "Accuracy:  0.875079333782196\n",
      "Validation Accuracy:  0.8711309432983398\n",
      "Iteration/Epoch:  740\n",
      "Accuracy:  0.8753967881202698\n",
      "Validation Accuracy:  0.8713690638542175\n",
      "Iteration/Epoch:  750\n",
      "Accuracy:  0.8757142424583435\n",
      "Validation Accuracy:  0.8717857003211975\n",
      "Iteration/Epoch:  760\n",
      "Accuracy:  0.8764682412147522\n",
      "Validation Accuracy:  0.8722618818283081\n",
      "Iteration/Epoch:  770\n",
      "Accuracy:  0.8771428465843201\n",
      "Validation Accuracy:  0.8726190328598022\n",
      "Iteration/Epoch:  780\n",
      "Accuracy:  0.8774603009223938\n",
      "Validation Accuracy:  0.8730952143669128\n",
      "Iteration/Epoch:  790\n",
      "Accuracy:  0.8778570890426636\n",
      "Validation Accuracy:  0.873452365398407\n",
      "Iteration/Epoch:  800\n",
      "Accuracy:  0.8780158162117004\n",
      "Validation Accuracy:  0.8741666674613953\n",
      "Iteration/Epoch:  810\n",
      "Accuracy:  0.8782936334609985\n",
      "Validation Accuracy:  0.8747023940086365\n",
      "Iteration/Epoch:  820\n",
      "Accuracy:  0.8786904215812683\n",
      "Validation Accuracy:  0.8750594854354858\n",
      "Iteration/Epoch:  830\n",
      "Accuracy:  0.8789682388305664\n",
      "Validation Accuracy:  0.875595211982727\n",
      "Iteration/Epoch:  840\n",
      "Accuracy:  0.879444420337677\n",
      "Validation Accuracy:  0.8759523630142212\n",
      "Iteration/Epoch:  850\n",
      "Accuracy:  0.8797618746757507\n",
      "Validation Accuracy:  0.8763095140457153\n",
      "Iteration/Epoch:  860\n",
      "Accuracy:  0.8798412084579468\n",
      "Validation Accuracy:  0.8767856955528259\n",
      "Iteration/Epoch:  870\n",
      "Accuracy:  0.8801587224006653\n",
      "Validation Accuracy:  0.8770238161087036\n",
      "Iteration/Epoch:  880\n",
      "Accuracy:  0.880476176738739\n",
      "Validation Accuracy:  0.8774404525756836\n",
      "Iteration/Epoch:  890\n",
      "Accuracy:  0.8809126615524292\n",
      "Validation Accuracy:  0.8774999976158142\n",
      "Iteration/Epoch:  900\n",
      "Accuracy:  0.8811904191970825\n",
      "Validation Accuracy:  0.8773809671401978\n",
      "Iteration/Epoch:  910\n",
      "Accuracy:  0.8814285397529602\n",
      "Validation Accuracy:  0.8779761791229248\n",
      "Iteration/Epoch:  920\n",
      "Accuracy:  0.8816666007041931\n",
      "Validation Accuracy:  0.8782142996788025\n",
      "Iteration/Epoch:  930\n",
      "Accuracy:  0.8821428418159485\n",
      "Validation Accuracy:  0.8783928751945496\n",
      "Iteration/Epoch:  940\n",
      "Accuracy:  0.8825396299362183\n",
      "Validation Accuracy:  0.878511905670166\n",
      "Iteration/Epoch:  950\n",
      "Accuracy:  0.8828967809677124\n",
      "Validation Accuracy:  0.8788095116615295\n",
      "Iteration/Epoch:  960\n",
      "Accuracy:  0.883293628692627\n",
      "Validation Accuracy:  0.8791666626930237\n",
      "Iteration/Epoch:  970\n",
      "Accuracy:  0.8834523558616638\n",
      "Validation Accuracy:  0.8794642686843872\n",
      "Iteration/Epoch:  980\n",
      "Accuracy:  0.8836110830307007\n",
      "Validation Accuracy:  0.8797023892402649\n",
      "Iteration/Epoch:  990\n",
      "Accuracy:  0.8839285373687744\n",
      "Validation Accuracy:  0.8799404501914978\n",
      "Iteration/Epoch:  1000\n",
      "Accuracy:  0.8842062950134277\n",
      "Validation Accuracy:  0.8802380561828613\n",
      "Iteration/Epoch:  1010\n",
      "Accuracy:  0.8844841122627258\n",
      "Validation Accuracy:  0.8807737827301025\n",
      "Iteration/Epoch:  1020\n",
      "Accuracy:  0.8849999904632568\n",
      "Validation Accuracy:  0.8810714483261108\n",
      "Iteration/Epoch:  1030\n",
      "Accuracy:  0.8856348991394043\n",
      "Validation Accuracy:  0.8813690543174744\n",
      "Iteration/Epoch:  1040\n",
      "Accuracy:  0.8859920501708984\n",
      "Validation Accuracy:  0.8815476298332214\n",
      "Iteration/Epoch:  1050\n",
      "Accuracy:  0.8862301111221313\n",
      "Validation Accuracy:  0.8817856907844543\n",
      "Iteration/Epoch:  1060\n",
      "Accuracy:  0.8864285349845886\n",
      "Validation Accuracy:  0.8823214173316956\n",
      "Iteration/Epoch:  1070\n",
      "Accuracy:  0.8865872621536255\n",
      "Validation Accuracy:  0.8826785683631897\n",
      "Iteration/Epoch:  1080\n",
      "Accuracy:  0.8871031403541565\n",
      "Validation Accuracy:  0.8829166293144226\n",
      "Iteration/Epoch:  1090\n",
      "Accuracy:  0.8873808979988098\n",
      "Validation Accuracy:  0.8829761743545532\n",
      "Iteration/Epoch:  1100\n",
      "Accuracy:  0.8878967761993408\n",
      "Validation Accuracy:  0.8832737803459167\n",
      "Iteration/Epoch:  1110\n",
      "Accuracy:  0.8881348967552185\n",
      "Validation Accuracy:  0.8835119009017944\n",
      "Iteration/Epoch:  1120\n",
      "Accuracy:  0.8884920477867126\n",
      "Validation Accuracy:  0.883809506893158\n",
      "Iteration/Epoch:  1130\n",
      "Accuracy:  0.8888095021247864\n",
      "Validation Accuracy:  0.8839285373687744\n",
      "Iteration/Epoch:  1140\n",
      "Accuracy:  0.8891269564628601\n",
      "Validation Accuracy:  0.8839285373687744\n",
      "Iteration/Epoch:  1150\n",
      "Accuracy:  0.8895237445831299\n",
      "Validation Accuracy:  0.883988082408905\n",
      "Iteration/Epoch:  1160\n",
      "Accuracy:  0.8899602890014648\n",
      "Validation Accuracy:  0.8842856884002686\n",
      "Iteration/Epoch:  1170\n",
      "Accuracy:  0.8899602890014648\n",
      "Validation Accuracy:  0.8847618699073792\n",
      "Iteration/Epoch:  1180\n",
      "Accuracy:  0.8902777433395386\n",
      "Validation Accuracy:  0.8849999904632568\n",
      "Iteration/Epoch:  1190\n",
      "Accuracy:  0.8906348943710327\n",
      "Validation Accuracy:  0.8852381110191345\n",
      "Iteration/Epoch:  1200\n",
      "Accuracy:  0.89083331823349\n",
      "Validation Accuracy:  0.8854761719703674\n",
      "Iteration/Epoch:  1210\n",
      "Accuracy:  0.8910713791847229\n",
      "Validation Accuracy:  0.885952353477478\n",
      "Iteration/Epoch:  1220\n",
      "Accuracy:  0.8911507725715637\n",
      "Validation Accuracy:  0.8861309289932251\n",
      "Iteration/Epoch:  1230\n",
      "Accuracy:  0.891428530216217\n",
      "Validation Accuracy:  0.8860714435577393\n",
      "Iteration/Epoch:  1240\n",
      "Accuracy:  0.8915872573852539\n",
      "Validation Accuracy:  0.8860714435577393\n",
      "Iteration/Epoch:  1250\n",
      "Accuracy:  0.8918650150299072\n",
      "Validation Accuracy:  0.8866071105003357\n",
      "Iteration/Epoch:  1260\n",
      "Accuracy:  0.8921428322792053\n",
      "Validation Accuracy:  0.8867262005805969\n",
      "Iteration/Epoch:  1270\n",
      "Accuracy:  0.8923015594482422\n",
      "Validation Accuracy:  0.8867856860160828\n",
      "Iteration/Epoch:  1280\n",
      "Accuracy:  0.8924999833106995\n",
      "Validation Accuracy:  0.8867856860160828\n",
      "Iteration/Epoch:  1290\n",
      "Accuracy:  0.8928174376487732\n",
      "Validation Accuracy:  0.8869642615318298\n",
      "Iteration/Epoch:  1300\n",
      "Accuracy:  0.8930951952934265\n",
      "Validation Accuracy:  0.8870833516120911\n",
      "Iteration/Epoch:  1310\n",
      "Accuracy:  0.8934920430183411\n",
      "Validation Accuracy:  0.8871428370475769\n",
      "Iteration/Epoch:  1320\n",
      "Accuracy:  0.8936507701873779\n",
      "Validation Accuracy:  0.8873809576034546\n",
      "Iteration/Epoch:  1330\n",
      "Accuracy:  0.8939682245254517\n",
      "Validation Accuracy:  0.8877381086349487\n",
      "Iteration/Epoch:  1340\n",
      "Accuracy:  0.8941269516944885\n",
      "Validation Accuracy:  0.8879761695861816\n",
      "Iteration/Epoch:  1350\n",
      "Accuracy:  0.8944444060325623\n",
      "Validation Accuracy:  0.8882737755775452\n",
      "Iteration/Epoch:  1360\n",
      "Accuracy:  0.894761860370636\n",
      "Validation Accuracy:  0.8885118961334229\n",
      "Iteration/Epoch:  1370\n",
      "Accuracy:  0.8951190114021301\n",
      "Validation Accuracy:  0.8885714411735535\n",
      "Iteration/Epoch:  1380\n",
      "Accuracy:  0.8952380418777466\n",
      "Validation Accuracy:  0.8886904716491699\n",
      "Iteration/Epoch:  1390\n",
      "Accuracy:  0.8951587080955505\n",
      "Validation Accuracy:  0.8889285326004028\n",
      "Iteration/Epoch:  1400\n",
      "Accuracy:  0.8953571319580078\n",
      "Validation Accuracy:  0.8889285326004028\n",
      "Iteration/Epoch:  1410\n",
      "Accuracy:  0.8955158591270447\n",
      "Validation Accuracy:  0.8891666531562805\n",
      "Iteration/Epoch:  1420\n",
      "Accuracy:  0.8957142233848572\n",
      "Validation Accuracy:  0.889285683631897\n",
      "Iteration/Epoch:  1430\n",
      "Accuracy:  0.8959523439407349\n",
      "Validation Accuracy:  0.8893452286720276\n",
      "Iteration/Epoch:  1440\n",
      "Accuracy:  0.8963491916656494\n",
      "Validation Accuracy:  0.8893452286720276\n",
      "Iteration/Epoch:  1450\n",
      "Accuracy:  0.8964682221412659\n",
      "Validation Accuracy:  0.889464259147644\n",
      "Iteration/Epoch:  1460\n",
      "Accuracy:  0.8965475559234619\n",
      "Validation Accuracy:  0.8897619247436523\n",
      "Iteration/Epoch:  1470\n",
      "Accuracy:  0.8967856764793396\n",
      "Validation Accuracy:  0.8898214101791382\n",
      "Iteration/Epoch:  1480\n",
      "Accuracy:  0.8971031308174133\n",
      "Validation Accuracy:  0.8901190161705017\n",
      "Iteration/Epoch:  1490\n",
      "Accuracy:  0.8971825242042542\n",
      "Validation Accuracy:  0.8904761672019958\n",
      "Iteration/Epoch:  1500\n",
      "Accuracy:  0.8974205851554871\n",
      "Validation Accuracy:  0.89041668176651\n",
      "Iteration/Epoch:  1510\n",
      "Accuracy:  0.8977777361869812\n",
      "Validation Accuracy:  0.89041668176651\n",
      "Iteration/Epoch:  1520\n",
      "Accuracy:  0.8980158567428589\n",
      "Validation Accuracy:  0.8905357122421265\n",
      "Iteration/Epoch:  1530\n",
      "Accuracy:  0.8982936143875122\n",
      "Validation Accuracy:  0.8905952572822571\n",
      "Iteration/Epoch:  1540\n",
      "Accuracy:  0.8984126448631287\n",
      "Validation Accuracy:  0.8907737731933594\n",
      "Iteration/Epoch:  1550\n",
      "Accuracy:  0.8986110687255859\n",
      "Validation Accuracy:  0.8907142877578735\n",
      "Iteration/Epoch:  1560\n",
      "Accuracy:  0.8987300992012024\n",
      "Validation Accuracy:  0.8908928632736206\n",
      "Iteration/Epoch:  1570\n",
      "Accuracy:  0.8988888263702393\n",
      "Validation Accuracy:  0.8909523487091064\n",
      "Iteration/Epoch:  1580\n",
      "Accuracy:  0.8990872502326965\n",
      "Validation Accuracy:  0.8911904692649841\n",
      "Iteration/Epoch:  1590\n",
      "Accuracy:  0.8992856740951538\n",
      "Validation Accuracy:  0.8913094997406006\n",
      "Iteration/Epoch:  1600\n",
      "Accuracy:  0.8994840979576111\n",
      "Validation Accuracy:  0.8914880752563477\n",
      "Iteration/Epoch:  1610\n",
      "Accuracy:  0.8996031284332275\n",
      "Validation Accuracy:  0.8915476202964783\n",
      "Iteration/Epoch:  1620\n",
      "Accuracy:  0.8995634317398071\n",
      "Validation Accuracy:  0.8917856812477112\n",
      "Iteration/Epoch:  1630\n",
      "Accuracy:  0.899722158908844\n",
      "Validation Accuracy:  0.8919047713279724\n",
      "Iteration/Epoch:  1640\n",
      "Accuracy:  0.899722158908844\n",
      "Validation Accuracy:  0.8921428322792053\n",
      "Iteration/Epoch:  1650\n",
      "Accuracy:  0.8998808860778809\n",
      "Validation Accuracy:  0.8922619223594666\n",
      "Iteration/Epoch:  1660\n",
      "Accuracy:  0.900158703327179\n",
      "Validation Accuracy:  0.892380952835083\n",
      "Iteration/Epoch:  1670\n",
      "Accuracy:  0.900238037109375\n",
      "Validation Accuracy:  0.892380952835083\n",
      "Iteration/Epoch:  1680\n",
      "Accuracy:  0.9003967642784119\n",
      "Validation Accuracy:  0.8926190137863159\n",
      "Iteration/Epoch:  1690\n",
      "Accuracy:  0.9005158543586731\n",
      "Validation Accuracy:  0.8930357098579407\n",
      "Iteration/Epoch:  1700\n",
      "Accuracy:  0.9007142782211304\n",
      "Validation Accuracy:  0.8931547403335571\n",
      "Iteration/Epoch:  1710\n",
      "Accuracy:  0.9009523391723633\n",
      "Validation Accuracy:  0.8931547403335571\n",
      "Iteration/Epoch:  1720\n",
      "Accuracy:  0.9011110663414001\n",
      "Validation Accuracy:  0.8930952548980713\n",
      "Iteration/Epoch:  1730\n",
      "Accuracy:  0.901269793510437\n",
      "Validation Accuracy:  0.8932142853736877\n",
      "Iteration/Epoch:  1740\n",
      "Accuracy:  0.9014682173728943\n",
      "Validation Accuracy:  0.8934523463249207\n",
      "Iteration/Epoch:  1750\n",
      "Accuracy:  0.9015476107597351\n",
      "Validation Accuracy:  0.8936309218406677\n",
      "Iteration/Epoch:  1760\n",
      "Accuracy:  0.901706337928772\n",
      "Validation Accuracy:  0.8938690423965454\n",
      "Iteration/Epoch:  1770\n",
      "Accuracy:  0.9019840955734253\n",
      "Validation Accuracy:  0.8938094973564148\n",
      "Iteration/Epoch:  1780\n",
      "Accuracy:  0.9019840955734253\n",
      "Validation Accuracy:  0.8939880728721619\n",
      "Iteration/Epoch:  1790\n",
      "Accuracy:  0.9022618532180786\n",
      "Validation Accuracy:  0.893928587436676\n",
      "Iteration/Epoch:  1800\n",
      "Accuracy:  0.9024602770805359\n",
      "Validation Accuracy:  0.8941071629524231\n",
      "Iteration/Epoch:  1810\n",
      "Accuracy:  0.9025396704673767\n",
      "Validation Accuracy:  0.8942856788635254\n",
      "Iteration/Epoch:  1820\n",
      "Accuracy:  0.9026190042495728\n",
      "Validation Accuracy:  0.8944642543792725\n",
      "Iteration/Epoch:  1830\n",
      "Accuracy:  0.9027380347251892\n",
      "Validation Accuracy:  0.8946428298950195\n",
      "Iteration/Epoch:  1840\n",
      "Accuracy:  0.9026983976364136\n",
      "Validation Accuracy:  0.8948214054107666\n",
      "Iteration/Epoch:  1850\n",
      "Accuracy:  0.9028967618942261\n",
      "Validation Accuracy:  0.8948809504508972\n",
      "Iteration/Epoch:  1860\n",
      "Accuracy:  0.9030158519744873\n",
      "Validation Accuracy:  0.8949999809265137\n",
      "Iteration/Epoch:  1870\n",
      "Accuracy:  0.9030951857566833\n",
      "Validation Accuracy:  0.8950595259666443\n",
      "Iteration/Epoch:  1880\n",
      "Accuracy:  0.9031745791435242\n",
      "Validation Accuracy:  0.8953571319580078\n",
      "Iteration/Epoch:  1890\n",
      "Accuracy:  0.9031745791435242\n",
      "Validation Accuracy:  0.8955357074737549\n",
      "Iteration/Epoch:  1900\n",
      "Accuracy:  0.9032142758369446\n",
      "Validation Accuracy:  0.8954761624336243\n",
      "Iteration/Epoch:  1910\n",
      "Accuracy:  0.903571367263794\n",
      "Validation Accuracy:  0.8955357074737549\n",
      "Iteration/Epoch:  1920\n",
      "Accuracy:  0.9036110639572144\n",
      "Validation Accuracy:  0.8955952525138855\n",
      "Iteration/Epoch:  1930\n",
      "Accuracy:  0.9036904573440552\n",
      "Validation Accuracy:  0.8958333134651184\n",
      "Iteration/Epoch:  1940\n",
      "Accuracy:  0.903849184513092\n",
      "Validation Accuracy:  0.8958333134651184\n",
      "Iteration/Epoch:  1950\n",
      "Accuracy:  0.9040079116821289\n",
      "Validation Accuracy:  0.8958333134651184\n",
      "Iteration/Epoch:  1960\n",
      "Accuracy:  0.9041269421577454\n",
      "Validation Accuracy:  0.8961904644966125\n",
      "Iteration/Epoch:  1970\n",
      "Accuracy:  0.9042063355445862\n",
      "Validation Accuracy:  0.8961309194564819\n",
      "Iteration/Epoch:  1980\n",
      "Accuracy:  0.9044443964958191\n",
      "Validation Accuracy:  0.8961904644966125\n",
      "Iteration/Epoch:  1990\n",
      "Accuracy:  0.9045634269714355\n",
      "Validation Accuracy:  0.8962500095367432\n",
      "Iteration/Epoch:  2000\n",
      "Accuracy:  0.9047222137451172\n",
      "Validation Accuracy:  0.896309494972229\n",
      "Iteration/Epoch:  2010\n",
      "Accuracy:  0.904880940914154\n",
      "Validation Accuracy:  0.8964880704879761\n",
      "Iteration/Epoch:  2020\n",
      "Accuracy:  0.905119001865387\n",
      "Validation Accuracy:  0.8964880704879761\n",
      "Iteration/Epoch:  2030\n",
      "Accuracy:  0.9053174257278442\n",
      "Validation Accuracy:  0.8967261910438538\n",
      "Iteration/Epoch:  2040\n",
      "Accuracy:  0.9053967595100403\n",
      "Validation Accuracy:  0.8969047665596008\n",
      "Iteration/Epoch:  2050\n",
      "Accuracy:  0.9054364562034607\n",
      "Validation Accuracy:  0.8969047665596008\n",
      "Iteration/Epoch:  2060\n",
      "Accuracy:  0.9055158495903015\n",
      "Validation Accuracy:  0.8969047665596008\n",
      "Iteration/Epoch:  2070\n",
      "Accuracy:  0.9057142734527588\n",
      "Validation Accuracy:  0.8969642519950867\n",
      "Iteration/Epoch:  2080\n",
      "Accuracy:  0.9058333039283752\n",
      "Validation Accuracy:  0.8969047665596008\n",
      "Iteration/Epoch:  2090\n",
      "Accuracy:  0.9059126377105713\n",
      "Validation Accuracy:  0.8971428275108337\n",
      "Iteration/Epoch:  2100\n",
      "Accuracy:  0.9061110615730286\n",
      "Validation Accuracy:  0.8971428275108337\n",
      "Iteration/Epoch:  2110\n",
      "Accuracy:  0.906150758266449\n",
      "Validation Accuracy:  0.897261917591095\n",
      "Iteration/Epoch:  2120\n",
      "Accuracy:  0.9062697887420654\n",
      "Validation Accuracy:  0.897440493106842\n",
      "Iteration/Epoch:  2130\n",
      "Accuracy:  0.9064285159111023\n",
      "Validation Accuracy:  0.8976190090179443\n",
      "Iteration/Epoch:  2140\n",
      "Accuracy:  0.9067063331604004\n",
      "Validation Accuracy:  0.8977380990982056\n",
      "Iteration/Epoch:  2150\n",
      "Accuracy:  0.9070634841918945\n",
      "Validation Accuracy:  0.8977380990982056\n",
      "Iteration/Epoch:  2160\n",
      "Accuracy:  0.9070237874984741\n",
      "Validation Accuracy:  0.897857129573822\n",
      "Iteration/Epoch:  2170\n",
      "Accuracy:  0.9070237874984741\n",
      "Validation Accuracy:  0.8981547355651855\n",
      "Iteration/Epoch:  2180\n",
      "Accuracy:  0.9071031212806702\n",
      "Validation Accuracy:  0.8980952501296997\n",
      "Iteration/Epoch:  2190\n",
      "Accuracy:  0.907182514667511\n",
      "Validation Accuracy:  0.8980357050895691\n",
      "Iteration/Epoch:  2200\n",
      "Accuracy:  0.9073412418365479\n",
      "Validation Accuracy:  0.8980952501296997\n",
      "Iteration/Epoch:  2210\n",
      "Accuracy:  0.9074602723121643\n",
      "Validation Accuracy:  0.8980952501296997\n",
      "Iteration/Epoch:  2220\n",
      "Accuracy:  0.9075396656990051\n",
      "Validation Accuracy:  0.8980357050895691\n",
      "Iteration/Epoch:  2230\n",
      "Accuracy:  0.9076189994812012\n",
      "Validation Accuracy:  0.8979166746139526\n",
      "Iteration/Epoch:  2240\n",
      "Accuracy:  0.9078174233436584\n",
      "Validation Accuracy:  0.8982142806053162\n",
      "Iteration/Epoch:  2250\n",
      "Accuracy:  0.9080555438995361\n",
      "Validation Accuracy:  0.8981547355651855\n",
      "Iteration/Epoch:  2260\n",
      "Accuracy:  0.908214271068573\n",
      "Validation Accuracy:  0.8982738256454468\n",
      "Iteration/Epoch:  2270\n",
      "Accuracy:  0.908214271068573\n",
      "Validation Accuracy:  0.8984523415565491\n",
      "Iteration/Epoch:  2280\n",
      "Accuracy:  0.9082539081573486\n",
      "Validation Accuracy:  0.8984523415565491\n",
      "Iteration/Epoch:  2290\n",
      "Accuracy:  0.9083729982376099\n",
      "Validation Accuracy:  0.8985714316368103\n",
      "Iteration/Epoch:  2300\n",
      "Accuracy:  0.9084920287132263\n",
      "Validation Accuracy:  0.8986309170722961\n",
      "Iteration/Epoch:  2310\n",
      "Accuracy:  0.9086904525756836\n",
      "Validation Accuracy:  0.8987500071525574\n",
      "Iteration/Epoch:  2320\n",
      "Accuracy:  0.9089285135269165\n",
      "Validation Accuracy:  0.8989880681037903\n",
      "Iteration/Epoch:  2330\n",
      "Accuracy:  0.9090079069137573\n",
      "Validation Accuracy:  0.8991071581840515\n",
      "Iteration/Epoch:  2340\n",
      "Accuracy:  0.9091269373893738\n",
      "Validation Accuracy:  0.8990476131439209\n",
      "Iteration/Epoch:  2350\n",
      "Accuracy:  0.9091666340827942\n",
      "Validation Accuracy:  0.8991071581840515\n",
      "Iteration/Epoch:  2360\n",
      "Accuracy:  0.9092459678649902\n",
      "Validation Accuracy:  0.8992857336997986\n",
      "Iteration/Epoch:  2370\n",
      "Accuracy:  0.9094443917274475\n",
      "Validation Accuracy:  0.8994642496109009\n",
      "Iteration/Epoch:  2380\n",
      "Accuracy:  0.9095634818077087\n",
      "Validation Accuracy:  0.8995833396911621\n",
      "Iteration/Epoch:  2390\n",
      "Accuracy:  0.9096428155899048\n",
      "Validation Accuracy:  0.899821400642395\n",
      "Iteration/Epoch:  2400\n",
      "Accuracy:  0.9096428155899048\n",
      "Validation Accuracy:  0.9000595211982727\n",
      "Iteration/Epoch:  2410\n",
      "Accuracy:  0.9097618460655212\n",
      "Validation Accuracy:  0.9001190662384033\n",
      "Iteration/Epoch:  2420\n",
      "Accuracy:  0.9098809361457825\n",
      "Validation Accuracy:  0.9002380967140198\n",
      "Iteration/Epoch:  2430\n",
      "Accuracy:  0.9099999666213989\n",
      "Validation Accuracy:  0.9003571271896362\n",
      "Iteration/Epoch:  2440\n",
      "Accuracy:  0.910079300403595\n",
      "Validation Accuracy:  0.9004166722297668\n",
      "Iteration/Epoch:  2450\n",
      "Accuracy:  0.910079300403595\n",
      "Validation Accuracy:  0.9005357027053833\n",
      "Iteration/Epoch:  2460\n",
      "Accuracy:  0.9102380871772766\n",
      "Validation Accuracy:  0.9004761576652527\n",
      "Iteration/Epoch:  2470\n",
      "Accuracy:  0.9102777242660522\n",
      "Validation Accuracy:  0.900773823261261\n",
      "Iteration/Epoch:  2480\n",
      "Accuracy:  0.9102777242660522\n",
      "Validation Accuracy:  0.900773823261261\n",
      "Iteration/Epoch:  2490\n",
      "Accuracy:  0.9103968143463135\n",
      "Validation Accuracy:  0.900773823261261\n"
     ]
    }
   ],
   "source": [
    "W1,b1,W2,b2= gradient_descent(X_train,Y_train,0.1,2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA is WORKING NICELY NICEEEEEEEEEE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
